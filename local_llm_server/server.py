import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from config import HF_API_KEY
from logger import local_server_logger  # Import the logger
import uvicorn

# Initialize FastAPI
app = FastAPI()

# Set model cache path to avoid re-downloading
CACHE_DIR = "./models"
flan_t5_model_name = "google/flan-t5-large"
flan_t5_model_path = os.path.join(CACHE_DIR, flan_t5_model_name)

# Ensure the cache directory exists
os.makedirs(CACHE_DIR, exist_ok=True)

try:
    # Load Flan-T5 Model and Tokenizer (from cache if exists)
    local_server_logger.info(f"Loading tokenizer from {flan_t5_model_name}")
    flan_t5_tokenizer = AutoTokenizer.from_pretrained(
        flan_t5_model_path if os.path.exists(flan_t5_model_path) else flan_t5_model_name,
        use_auth_token=HF_API_KEY
    )
    
    local_server_logger.info(f"Loading model from {flan_t5_model_name}")
    flan_t5_model = AutoModelForSeq2SeqLM.from_pretrained(
        flan_t5_model_path if os.path.exists(flan_t5_model_path) else flan_t5_model_name,
        use_auth_token=HF_API_KEY
    )
    local_server_logger.info("Model and tokenizer loaded successfully")
except Exception as e:
    local_server_logger.error(f"Error loading model: {e}")
    raise

# Define a request model for prompt input
class Request(BaseModel):
    prompt: str

# Status Endpoint
@app.get("/status")
async def get_status():
    return {"status": "running"}

# Flan-T5 Model Route
@app.get("/flan-t5-large/model")
async def get_model_info():
    return {"model": flan_t5_model_name}

# Flan-T5 Endpoint
@app.post("/flan-t5-large/query_pdf")
async def query_pdf(request: Request):
    try:
        # Debug logging
        local_server_logger.info(f"Received request: {request}")
        
        # Process the request
        inputs = flan_t5_tokenizer(request.prompt, return_tensors="pt")
        outputs = flan_t5_model.generate(
            inputs["input_ids"], 
            max_new_tokens=500,
            temperature=0.7,  # Add temperature for more varied responses
            num_return_sequences=2,
            do_sample=True,
            min_length=10,       # Set minimum length
            num_beams=4,        # Use beam search
            no_repeat_ngram_size=2,  # Avoid repetition
            early_stopping=True
        )
        response = flan_t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Debug logging
        local_server_logger.info(f"Generated response: {response}")
        
        if not response.strip():  # Check if response is empty or just whitespace
            local_server_logger.warning("Empty response generated by model")
            return {"response": "No response generated. Please try rephrasing your question."}
        
        # Post-process the response
        response = response.strip()
        response = response.capitalize()
        if not response.endswith('.'):
            response += '.'
            
        return {"response": response}
    except Exception as e:
        local_server_logger.error(f"Error processing request: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Run the application
if __name__ == "__main__":
    local_server_logger.info("Starting FastAPI server")
    uvicorn.run(app, host="127.0.0.1", port=8000)